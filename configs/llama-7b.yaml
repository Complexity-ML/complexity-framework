# Llama 2 7B Configuration
# Reference: https://arxiv.org/abs/2307.09288

# Architecture
hidden_size: 4096
num_hidden_layers: 32
num_attention_heads: 32
num_key_value_heads: 32  # MHA (Llama 2 7B uses full MHA)
intermediate_size: 11008
vocab_size: 32000

# Attention
attention_type: mha
attention_dropout: 0.0
use_qk_norm: false
sliding_window: null  # Full attention

# Position
max_position_embeddings: 4096
rope_theta: 10000.0
rope_type: standard

# MLP
mlp_type: swiglu
hidden_act: silu
num_experts: 1

# Normalization
norm_type: rmsnorm
norm_eps: 1e-5

# Other
tie_word_embeddings: false
use_sdpa: true
initializer_range: 0.02
