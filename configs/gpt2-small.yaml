# GPT-2 Small Configuration
# Classic architecture for reference/comparison

# Architecture
hidden_size: 768
num_hidden_layers: 12
num_attention_heads: 12
num_key_value_heads: 12  # MHA
intermediate_size: 3072
vocab_size: 50257

# Attention
attention_type: mha
attention_dropout: 0.1
use_qk_norm: false
sliding_window: null

# Position
max_position_embeddings: 1024
rope_theta: 10000.0
rope_type: standard

# MLP
mlp_type: gelu
hidden_act: gelu
num_experts: 1

# Normalization
norm_type: layernorm
norm_eps: 1e-5

# Other
tie_word_embeddings: true
use_sdpa: true
initializer_range: 0.02
