# Complexity 7B Configuration
# Framework-Complexity default architecture with Token-Routed MoE

# Architecture
hidden_size: 4096
num_hidden_layers: 32
num_attention_heads: 32
num_key_value_heads: 8  # GQA
intermediate_size: 11008
vocab_size: 100000

# Attention
attention_type: gqa
attention_dropout: 0.0
use_qk_norm: true  # QK Normalization for stability
sliding_window: null  # Full attention

# Position
max_position_embeddings: 8192
rope_theta: 10000.0
rope_type: standard

# MLP - Token-Routed MoE (Complexity innovation!)
mlp_type: token_routed
hidden_act: silu
num_experts: 4  # 4 experts, deterministic routing

# Normalization
norm_type: rmsnorm
norm_eps: 1e-6

# Other
tie_word_embeddings: true
use_sdpa: true
initializer_range: 0.02
